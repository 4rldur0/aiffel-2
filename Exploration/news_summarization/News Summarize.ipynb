{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ccb508",
   "metadata": {},
   "source": [
    "# 프로젝트: 뉴스기사 요약해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0745ac47",
   "metadata": {},
   "source": [
    "from importlib.metadata import version\n",
    "import nltk\n",
    "import tensorflow\n",
    "import summa\n",
    "import pandas\n",
    "\n",
    "print(nltk.__version__)\n",
    "print(tensorflow.__version__)\n",
    "print(pandas.__version__)\n",
    "print(version('summa'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2bf3d2f",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5e1dbe10",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a88052",
   "metadata": {},
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n",
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ba5780e",
   "metadata": {},
   "source": [
    "data.sample(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "766c396c",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 전처리하기 (추상적 요약)\n",
    "\n",
    "실습에서 사용된 전처리를 참고하여 각자 필요하다고 생각하는 전처리를 추가 사용하여 텍스트를 정규화 또는 정제해 보세요. 만약, 불용어 제거를 선택한다면 상대적으로 길이가 짧은 요약 데이터에 대해서도 불용어를 제거하는 것이 좋을지 고민해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbb515ee",
   "metadata": {},
   "source": [
    "data.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd6a920e",
   "metadata": {},
   "source": [
    "print('headlines 열에서 중복을 배제한 유일한 샘플의 수 :', data['headlines'].nunique())\n",
    "print('text 열에서 중복을 배제한 유일한 샘플의 수 :', data['text'].nunique())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7ea6535",
   "metadata": {},
   "source": [
    "data.drop_duplicates(subset = ['text'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c5e5837",
   "metadata": {},
   "source": [
    "print(data.isnull().sum())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fa407fa4",
   "metadata": {},
   "source": [
    "## 텍스트 정규화와 불용어 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc35d2",
   "metadata": {},
   "source": [
    "- 직접 눈으로 단어를 몇개 보면서 전처리 방식을 수정하였다.\n",
    "-  '&' -> ' and '로,  \\' -> '로 교체\n",
    "- A&B일 경우를 고려해 ' and '로 전처리 한것을 감안해 다중 공백 제거하는 전처리를 추가하였다\n",
    "- 불용어는 no나 not 같은 부정어들까지 사라지게 하므로 불용어를 리스트로 가져와서 no와 not을 빼고 stop_words로 만들어주었다\n",
    "    - 처음에 stopwords를 아에 빼고 전처리를 했으나 성능이 상당히 떨어졌기에 이렇게 전처리를 변경하였다\n",
    "    - 마지막 결과에 stopwords 적용후와 전의 성능차이를 볼 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34868de2",
   "metadata": {},
   "source": [
    "data.sample(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40800b37",
   "metadata": {},
   "source": [
    "data.headlines.sample(5).tolist()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ad5ad6b",
   "metadata": {},
   "source": [
    "data.text.sample(5).tolist()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac7ab68b",
   "metadata": {},
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2bbcc7eb",
   "metadata": {},
   "source": [
    "## Stopwords 후처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21cf06f1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9093b64a",
   "metadata": {},
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('not')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ae9896c",
   "metadata": {},
   "source": [
    "print(stop_words)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2649c35d",
   "metadata": {},
   "source": [
    "from tqdm import tqdm, trange\n",
    "\n",
    "tqdm.pandas()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e6a9845",
   "metadata": {},
   "source": [
    "# 데이터 전처리 함수\n",
    "\n",
    "def preprocess_sentence(sentence, remove_stopwords=True, stopwords=[]):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\\\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    \n",
    "    sentence = re.sub(\"&\",\" and \", sentence) # & -> and로 교체\n",
    "    sentence = re.sub(\"\\'\",\"'\", sentence) # \\' -> '로 교체\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    sentence = re.sub(' +', ' ', sentence) # 다중공백 제거\n",
    "    # sentence = ' '.join(sentence.split())\n",
    "     \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stop_words if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21191487",
   "metadata": {},
   "source": [
    "clean_text = data['text'].progress_apply(lambda x: preprocess_sentence(x, remove_stopwords=True, stopwords=stop_words)).tolist()\n",
    "\n",
    "print(\"text 전처리 후 결과: \", clean_text[:5])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54c39dcb",
   "metadata": {},
   "source": [
    "clean_headlines = data['headlines'].progress_apply(lambda x: preprocess_sentence(x, remove_stopwords=True, stopwords=stop_words)).tolist()\n",
    "\n",
    "print(\"headline 전처리 후 결과: \", clean_headlines[:5])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee33916a",
   "metadata": {},
   "source": [
    "# 원본을 news에 저장해두고 오류 발생시 다시 사용하고자 한다\n",
    "news = data.copy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2afd4b73",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "data['text'] = clean_text\n",
    "data['headlines'] = clean_headlines\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8dadd2d",
   "metadata": {},
   "source": [
    "news"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c19e2829",
   "metadata": {},
   "source": [
    "data.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7057cccf",
   "metadata": {},
   "source": [
    "data.isnull().sum()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d76cb0f3",
   "metadata": {},
   "source": [
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['text']]\n",
    "summary_len = [len(s.split()) for s in data['headlines']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Headlines')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Headlines')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7bcdadea",
   "metadata": {},
   "source": [
    "### 이상치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "145dc6cc",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "np.argmin(text_len)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc64fda3",
   "metadata": {},
   "source": [
    "data.iloc[52]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "517c047f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "np.quantile(text_len, 0.25)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8297e97e",
   "metadata": {},
   "source": [
    "data.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b61b4312",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "data.drop(52, inplace=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27abc7f5",
   "metadata": {},
   "source": [
    "data.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0cd4bd68",
   "metadata": {},
   "source": [
    "np.argmin(summary_len)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f9372c3",
   "metadata": {},
   "source": [
    "data.iloc[np.argmin(summary_len)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b87cd7ad",
   "metadata": {},
   "source": [
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['text']]\n",
    "summary_len = [len(s.split()) for s in data['headlines']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Headlines')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Headlines')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a02fc4b0",
   "metadata": {},
   "source": [
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "902ca70d",
   "metadata": {},
   "source": [
    "text_max_len = 50\n",
    "summary_max_len = 10"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3277e691",
   "metadata": {},
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if(len(s.split()) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22989e4d",
   "metadata": {},
   "source": [
    "below_threshold_len(text_max_len, data['text'])\n",
    "below_threshold_len(summary_max_len,  data['headlines'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f9b190d5",
   "metadata": {},
   "source": [
    "### text_max_len과 summary_max_len의 길이보다 큰 샘플을 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7d8b6c2",
   "metadata": {},
   "source": [
    "data.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79d7778a",
   "metadata": {},
   "source": [
    "c1 = data['headlines'].apply(lambda x: len(x.split())) <= summary_max_len\n",
    "c2 = data['text'].apply(lambda x: len(x.split())) <= text_max_len\n",
    "\n",
    "data = data.loc[c1 & c2]\n",
    "\n",
    "print('전체 샘플수 :', (len(data)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "87b0aead",
   "metadata": {},
   "source": [
    "### 시작 토큰과 종료 토큰 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6cd9c68",
   "metadata": {},
   "source": [
    "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "data['decoder_input'] = data['headlines'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['headlines'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28e3b825",
   "metadata": {},
   "source": [
    "encoder_input = np.array(data['text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29746a5e",
   "metadata": {},
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5535595b",
   "metadata": {},
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e77482b",
   "metadata": {},
   "source": [
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72f5a2e6",
   "metadata": {},
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "495f366d",
   "metadata": {},
   "source": [
    "### 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "287cf09e",
   "metadata": {},
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "099d6ebf",
   "metadata": {},
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7254f2da",
   "metadata": {},
   "source": [
    "src_vocab = 20000\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 20,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18a02925",
   "metadata": {},
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e3286641",
   "metadata": {},
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "57629963",
   "metadata": {},
   "source": [
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "318217be",
   "metadata": {},
   "source": [
    "tar_vocab = 10000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7667f329",
   "metadata": {},
   "source": [
    "decoder_target_test[:5]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "50a1a627",
   "metadata": {},
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
    "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
    "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
    "\n",
    "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
    "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
    "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9c278b03",
   "metadata": {},
   "source": [
    "### 패딩하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a361eb",
   "metadata": {},
   "source": [
    "pre로 하면 어떻게 될까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e43f2a9",
   "metadata": {},
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a399d995",
   "metadata": {},
   "source": [
    "## 모델 설계하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f1111",
   "metadata": {},
   "source": [
    "- __모델 선정 이유__ \n",
    "    - lstm은 자연어 처리에서 대체로 좋은 성능을 내므로 lstm모델을 선정했고\n",
    "    - 요약 모델을 할때 seq2seq를 사용하므로 이를 활용하였고 \n",
    "    - attention은 lstm의 long term dependency 문제를 해결하기 위해 활용하였다\n",
    "    \n",
    "- __Metrics 선정 이유__\n",
    "    - 요약 모델의 평가시 Rouge score를 많이 활용한다하여 이를 사용해보고자 하였으나 시간상의 이유로 활용하지 못했고\n",
    "    - 대신에 직접 샘플을 통해 비교를 진행하였다\n",
    "    \n",
    "- __Loss 선정 이유__ \n",
    "    - sparse categorical crossentropy\n",
    "    - seq2seq는 결국 10000개의 단어중에서 다음 단어를 예측하는 문제로 바뀌므로 이에따라 sparse categorical crossentropy를 선정하였다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f9e7fadb",
   "metadata": {},
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "# encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "02e39924",
   "metadata": {},
   "source": [
    "디코더 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e144edd8",
   "metadata": {},
   "source": [
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "# decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b792b84",
   "metadata": {},
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "20afda59",
   "metadata": {},
   "source": [
    "### Step 3. 어텐션 메커니즘 사용하기 (추상적 요약)\n",
    "일반적인 seq2seq보다는 어텐션 메커니즘을 사용한 seq2seq를 사용하는 것이 더 나은 성능을 얻을 수 있어요. 실습 내용을 참고하여 어텐션 메커니즘을 사용한 seq2seq를 설계해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1d7e80bb",
   "metadata": {},
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AdditiveAttention(name='attention_layer')\n",
    "\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "337fb3d2",
   "metadata": {},
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dfd892b0",
   "metadata": {},
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b89762db",
   "metadata": {},
   "source": [
    "- 그래프를 통해 train loss와 val loss가 감소하는 것을 볼 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6b12d6",
   "metadata": {},
   "source": [
    "### Step 4. 실제 결과와 요약문 비교하기 (추상적 요약)\n",
    "원래의 요약문(headlines 열)과 학습을 통해 얻은 추상적 요약의 결과를 비교해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2d71558c",
   "metadata": {},
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f666e921",
   "metadata": {},
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0bfb686b",
   "metadata": {},
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "76bcec1e",
   "metadata": {},
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if (sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence.strip()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e6f5cfee",
   "metadata": {},
   "source": [
    "모델 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "49c141fc",
   "metadata": {},
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i>2):\n",
    "            temp = temp + tar_index_to_word[i]+' '\n",
    "    return temp.strip()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c9fd64ac",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "len(encoder_input_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3a863ef6",
   "metadata": {},
   "source": [
    "for i in range(50, 100):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e0a6b3d8",
   "metadata": {},
   "source": [
    "encoder_input_test.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5bb9a2a0",
   "metadata": {},
   "source": [
    "summerize_test = []\n",
    "for i in trange(len(encoder_input_test)):\n",
    "    tmp = decode_sequence(encoder_input_test[i].reshape(1, text_max_len))\n",
    "    summerize_test.append(tmp)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2de5dda4",
   "metadata": {},
   "source": [
    "- stopwords를 제거하지 않았더니 상당히 잘 요약하는 것으로 보인다\n",
    "- 모든 test_data를 번역한 뒤에 이를 활용해서 ROUGE SCORE를 구해보려 했으나 시간이 너무 오래 걸려서 하지 못했다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dbacf4bc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "len(summerize_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "65adc32d",
   "metadata": {},
   "source": [
    "### Step 5. Summa을 이용해서 추출적 요약해보기\n",
    "추상적 요약은 추출적 요약과는 달리 문장의 표현력을 다양하게 가져갈 수 있지만, 추출적 요약에 비해서 난이도가 높아요. 반대로 말하면 추출적 요약은 추상적 요약에 비해 난이도가 낮고 기존 문장에서 문장을 꺼내오는 것이므로 잘못된 요약이 나올 가능성이 낮아요.\n",
    "\n",
    "Summa의 summarize를 사용하여 추출적 요약을 해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc9c739",
   "metadata": {},
   "source": [
    "- 너무 적은 문장의 경우에는 요약을 못한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ef3aa65c",
   "metadata": {},
   "source": [
    "import requests\n",
    "from summa.summarizer import summarize"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ba11615e",
   "metadata": {},
   "source": [
    "text = news.text[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ad9ae9cf",
   "metadata": {},
   "source": [
    "text"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "675f532e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, words=15))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "91ba33e4",
   "metadata": {},
   "source": [
    "news['summa_summarize'] = news['text'].progress_apply(lambda x: summarize(x, words=15))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f1a6ac61",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "news['summa_summarize'].iloc[-n_of_val:]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ab9ddcf9",
   "metadata": {},
   "source": [
    "news.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "71318053",
   "metadata": {},
   "source": [
    "data.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3e3fafc2",
   "metadata": {},
   "source": [
    "all_data = data.merge(news, how='left', right_index=True, left_index=True, suffixes=['','_raw'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d384bbab",
   "metadata": {},
   "source": [
    "all_data.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dae2a4fd",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "test_data = all_data.iloc[-n_of_val:]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "51809f56",
   "metadata": {},
   "source": [
    "samples = test_data.sample(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "62686dab",
   "metadata": {},
   "source": [
    "samples"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4207af00",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca916739",
   "metadata": {},
   "source": [
    "- raw 문장에서 한번에 예측까지 하는 함수를 만들었고 이를 활용하여 전체 예측과정을 수행하였다\n",
    "- 완전 새로운 뉴스데이터를 가져오려하였으나 용량 및 길이등의 문제로 test data(validation) 에서 샘플링을 하였다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c18f52d0",
   "metadata": {},
   "source": [
    "def make_summary(text):\n",
    "    pp = preprocess_sentence(text, False)\n",
    "    ppd = src_tokenizer.texts_to_sequences([pp])\n",
    "    ps = pad_sequences(ppd, maxlen=text_max_len, padding='post')\n",
    "    result = decode_sequence(ps.reshape(1, text_max_len))\n",
    "    return result"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1e87c9b8",
   "metadata": {},
   "source": [
    "results = []\n",
    "for text in samples.text_raw:\n",
    "    result = make_summary(text)\n",
    "    results.append(result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8aa3796c",
   "metadata": {},
   "source": [
    "samples['summary'] = results"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2459b122",
   "metadata": {},
   "source": [
    "samples.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fbc98a9c",
   "metadata": {},
   "source": [
    "for i in range(10):\n",
    "    print(\"원문 :\", samples.text_raw.tolist()[i])\n",
    "    print(\"실제 요약 :\", samples.headlines.tolist()[i])\n",
    "    print(\"예측 요약 :\", samples.summary.tolist()[i])\n",
    "    print(\"추출적 요약 :\", samples.summa_summarize.tolist()[i])\n",
    "    print(\"\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "57a51f27",
   "metadata": {},
   "source": [
    "!pip install rouge-score"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "512bd06c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from rouge_score import rouge_scorer"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0cd4d069",
   "metadata": {},
   "source": [
    "reference_summaries = samples.headlines.tolist()\n",
    "generated_summaries = samples.summary.tolist()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "53341063",
   "metadata": {},
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for ref, gen in zip(reference_summaries, generated_summaries):\n",
    "    scores = scorer.score(ref, gen)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "print(\"Average ROUGE-1: \", np.mean(rouge1_scores))\n",
    "print(\"Average ROUGE-2: \", np.mean(rouge2_scores))\n",
    "print(\"Average ROUGE-L: \", np.mean(rougeL_scores))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "66549d84",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "len(encoder_input_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b5ccce4a",
   "metadata": {},
   "source": [
    "len(encoder_input_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fc189eab",
   "metadata": {},
   "source": [
    "reference_summaries = []\n",
    "generated_summaries = []\n",
    "\n",
    "for i in trange(200, 300):\n",
    "    real = seq2summary(decoder_input_test[i])\n",
    "    generated = decode_sequence(encoder_input_test[i].reshape(1, text_max_len))\n",
    "    \n",
    "    reference_summaries.append(real)\n",
    "    generated_summaries.append(generated)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3a15cddb",
   "metadata": {},
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for ref, gen in zip(reference_summaries, generated_summaries):\n",
    "    scores = scorer.score(ref, gen)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "print(\"Average ROUGE-1: \", np.mean(rouge1_scores))\n",
    "print(\"Average ROUGE-2: \", np.mean(rouge2_scores))\n",
    "print(\"Average ROUGE-L: \", np.mean(rougeL_scores))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "695f2350",
   "metadata": {},
   "source": [
    "# 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f31ab97",
   "metadata": {},
   "source": [
    "### 결과 해석 (stopwords 적용전)\n",
    "- Average ROUGE-1: 0.101\n",
    "- Average ROUGE-2: 0.0105\n",
    "- Average ROUGE-L: 0.089"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67104736",
   "metadata": {},
   "source": [
    "### 눈으로 보고 결과 확인하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a8f14d",
   "metadata": {},
   "source": [
    "- 원문 : The Union government on Monday launched 'e-CinePramaan', the Online Film Certification System of the Central Board of Film Certification. This will make the process for films to obtain a certificate fully digital and will include QR Codes to check fraudulent certificates. \"A highlight of online film certification is elimination of middlemen and of corruption,\" said I&B Minister M Venkaiah Naidu.\n",
    "- 실제 요약 : govt launches online film certification system\n",
    "- 예측 요약 : govt to sell child accident in delhi ncr\n",
    "-----\n",
    "- 원문 : India is currently drafting a policy to safeguard its interests in Antarctica and will most likely present it at the winter session of the Parliament, reports said. Being a consultative member of the Antarctic Treaty, it is important for India to have clear directives to guide its activities in the region, officials said.\n",
    "- 실제 요약 : india to draft law to safeguard interests in antarctica\n",
    "- 예측 요약 : india to be part of doing business due to lack of hotels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b31bfb",
   "metadata": {},
   "source": [
    "### 결과 해석 (stopwords 적용결과)\n",
    "\n",
    "#### Average ROUGE-1: 0.2527\n",
    "\n",
    "    - 해석: 생성된 요약에서 참조 요약과 단어 단위로 25.27% 일치합니다.\n",
    "    - 의미: 단어 단위로 약 25.27%의 단어가 참조 요약과 일치한다는 것을 의미합니다. \n",
    "        - 이는 비교적 적당한 일치율로, 생성된 요약이 참조 요약과 단어 수준에서 어느 정도 유사성을 가지고 있음을 나타냅니다.\n",
    "\n",
    "#### Average ROUGE-2: 0.0336\n",
    "\n",
    "    - 해석: 생성된 요약에서 참조 요약과 2-그램(단어 쌍) 단위로 3.36% 일치합니다.\n",
    "    - 의미: 2-그램의 일치율이 3.36%로, 생성된 요약이 참조 요약과의 문장 구조나 연속된 단어 쌍을 비교적 낮게 반영하고 있음을 의미합니다. \n",
    "        - 이는 문장 구조와 어순에서 참조 요약과의 유사성이 낮음을 시사합니다.\n",
    "\n",
    "#### Average ROUGE-L: 0.2108\n",
    "\n",
    "    - 해석: 생성된 요약에서 참조 요약과 최장 공통 부분열(Longest Common Subsequence, LCS) 단위로 21.08% 일치합니다.\n",
    "    - 의미: LCS 기반의 일치율이 21.08%라는 것은, 생성된 요약이 참조 요약과 긴 부분열에서 어느 정도 일치하는 경우가 있다는 것을 의미합니다. \n",
    "        - 이는 문장의 중요한 흐름이나 구조를 어느 정도 유지하고 있음을 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61bbbb5",
   "metadata": {},
   "source": [
    "### 눈으로 보고 결과 확인하기 (stopwords 적용 결과)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f74f7",
   "metadata": {},
   "source": [
    "- 원문 : American computer technology major Oracle's 72-year-old Co-founder, Larry Ellison's net worth has jumped by $5 billion in two days to hit $61.8 billion. The jump in his net worth came from his around 25% stake in Oracle, after his company's shares soared following fourth-quarter earnings that exceeded analysts' expectations. Ellison is currently the seventh richest person in the world.\n",
    "- 실제 요약 : oracle founder larry ellison gets bn richer days\n",
    "- 예측 요약 : net net worth crore yr\n",
    "- 추출적 요약 : American computer technology major Oracle's 72-year-old Co-founder, Larry Ellison's net worth has jumped by $5 billion in two days to hit $61.8 billion.\n",
    "\n",
    "----\n",
    "- 원문 : Founder of McAfee anti-virus software John McAfee has tweeted that he will \"eat his d**k on national television\" if the price of Bitcoin doesn't cross $500,000 (about Ã¢ÂÂ¹3.2 crore) in three years. The digital currency Bitcoin reached a high of $3,018 in mid-June but dropped below $2,000 over the past weekend. The overall cryptocurrency market hit $80 billion on Tuesday.\n",
    "- 실제 요약 : eat tv bitcoin not hit cr mcafee\n",
    "- 예측 요약 : bitcoin could hit bn due power mcafee\n",
    "- 추출적 요약 : \n",
    "\n",
    "\n",
    "---\n",
    "- 원문 : Two friends Archit Gupta and Manish Mulchandani from Indore have secured All India Ranks 2 and 3 in the 2017 National Eligibility-cum-Entrance Test (NEET). Both Gupta and Mulchandani left their homes to live in a hostel to concentrate on their studies and encourage each other. \"...it is a healthy competition and it helped us both in scoring well,\" Mulchandani said.\n",
    "- 실제 요약 : two friends indore grab india ranks neet\n",
    "- 예측 요약 : india jee main\n",
    "- 추출적 요약 : Both Gupta and Mulchandani left their homes to live in a hostel to concentrate on their studies and encourage each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dbea6c",
   "metadata": {},
   "source": [
    "| 구분 | 추상적 요약 방식  | 추출적 요약 방식  |\n",
    "|-----|---|---|\n",
    "|문법완성도| 문법 완성도가 높지 않다 | 문법 완성도 있는 문장이 보여진다 |\n",
    "|핵심단어 포함| 핵심단어가 꽤 포함되는 것으로 보인다. <br> paraphrase 되어서 유사어가 나오는 경우도 볼 수 있었다 | 핵심단어가 포함된다  |\n",
    "| 장점 | 다양한 단어들을 볼 수 있다. 요약 길이 조절이 편하다 | 문법 완성도 있는 문장이 만들어진다 |\n",
    "| 단점 | 모델이 아직 정확도가 떨어지는 것으로 보인다 <br> 같은 단어가 반복되어 나오는 경우가 많다 | 뉴스의 길이가 일정 길이 이상이 되어야 요약이 된다  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82681e9b",
   "metadata": {},
   "source": [
    "## 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce07938",
   "metadata": {},
   "source": [
    "- __배운 점__\n",
    "    >- 자연어 요약 모델링을 했다\n",
    "    >- 전체 과정을 수행하면서 실제로 어떤 구조로 이루어져 있는지 알 수 있었다\n",
    "    >- stopwords 적용 전과 후의 차이가 많다는 것을 알 수 있었다\n",
    "    \n",
    "- __아쉬운 점__\n",
    "    >- 끝내는 데 급급해서 좀 더 자세하게 모델을 파악하지 못한 점이 아쉽다\n",
    "    >- 정교한 전처리, 모델링을 해보지 못해서 아쉽다\n",
    "    >- 성능이 많이 떨어져서 아쉽다\n",
    "- __느낀 점__\n",
    "    >- 자연어 처리 모델링은 너무 어렵다\n",
    "    >- 좀 더 꼼꼼하게 공부를 해야할 것 같다\n",
    "- __어려웠던 점__\n",
    "    >- summa summarize를 사용하는데 너무 짧은 문장은 요약이 안되는 것 같았다.\n",
    "    \n",
    "    \n",
    "- __추가 사항__\n",
    "    >- summa에서 키워드 추출을 해봐야겠다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
